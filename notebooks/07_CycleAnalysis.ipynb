{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07_cycle_analyzer.ipynb - Breathing Cycle Analysis\n",
    "\n",
    "This notebook analyzes the 4D breathing cycle data to understand:\n",
    "1. Data structure and consistency\n",
    "2. Motion patterns between phases\n",
    "3. Cycle timing and characteristics\n",
    "4. Quality assessment for interpolation\n",
    "\n",
    "**Output**: Analysis results for implementing frame interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfe2b5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e2cd97d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cycles root: /mnt/tcia_data/processed/4D-Lung-Cycles\n",
      "Output directory: /mnt/tcia_data/processed/4D-Lung-Cycles/analysis\n",
      "Cycles root exists: True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import cv2\n",
    "from scipy import ndimage\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "CYCLES_ROOT = Path(\"/mnt/tcia_data/processed/4D-Lung-Cycles/\")\n",
    "OUTPUT_DIR = Path(\"/mnt/tcia_data/processed/4D-Lung-Cycles/analysis\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Cycles root: {CYCLES_ROOT}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"Cycles root exists: {CYCLES_ROOT.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf80fa5",
   "metadata": {},
   "source": [
    "## 1. Data Structure Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39a2a4e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23 patient directories\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning patients: 100%|██████████| 23/23 [00:07<00:00,  3.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Structure analysis complete: 587 cycles found\n",
      "    patient_id          cycle_id  num_phases     volume_shape   dtype  \\\n",
      "0  116_HM10395  116_HM10395_S106          10   (50, 512, 512)   int16   \n",
      "1  116_HM10395  116_HM10395_S107          10   (50, 512, 512)   int16   \n",
      "2  116_HM10395  116_HM10395_S301          10  (118, 512, 512)  uint16   \n",
      "3  116_HM10395  116_HM10395_S118          10   (50, 512, 512)   int16   \n",
      "4  116_HM10395  116_HM10395_S102          10   (50, 512, 512)   int16   \n",
      "\n",
      "   file_size_mb  \n",
      "0    250.001221  \n",
      "1    250.001221  \n",
      "2    590.001221  \n",
      "3    250.001221  \n",
      "4    250.001221  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Scan directory structure\n",
    "def scan_data_structure():\n",
    "    \"\"\"Scan and analyze the data structure\"\"\"\n",
    "    \n",
    "    structure_info = []\n",
    "    \n",
    "    if not CYCLES_ROOT.exists():\n",
    "        print(f\"ERROR: {CYCLES_ROOT} does not exist\")\n",
    "        return None\n",
    "    \n",
    "    # Get all patient directories\n",
    "    patient_dirs = [d for d in CYCLES_ROOT.iterdir() if d.is_dir()]\n",
    "    print(f\"Found {len(patient_dirs)} patient directories\")\n",
    "    \n",
    "    for patient_dir in tqdm(patient_dirs, desc=\"Scanning patients\"):\n",
    "        patient_id = patient_dir.name\n",
    "        \n",
    "        # Get all cycle directories for this patient\n",
    "        cycle_dirs = [d for d in patient_dir.iterdir() if d.is_dir()]\n",
    "        \n",
    "        for cycle_dir in cycle_dirs:\n",
    "            cycle_id = cycle_dir.name\n",
    "            \n",
    "            # Get all phase files\n",
    "            phase_files = sorted(cycle_dir.glob(\"phase_*.npy\"))\n",
    "            \n",
    "            if not phase_files:\n",
    "                continue\n",
    "                \n",
    "            # Load first phase to get shape info\n",
    "            try:\n",
    "                first_phase = np.load(phase_files[0])\n",
    "                \n",
    "                structure_info.append({\n",
    "                    'patient_id': patient_id,\n",
    "                    'cycle_id': cycle_id,\n",
    "                    'num_phases': len(phase_files),\n",
    "                    'volume_shape': first_phase.shape,\n",
    "                    'dtype': str(first_phase.dtype),\n",
    "                    'file_size_mb': sum(f.stat().st_size for f in phase_files) / (1024**2)\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {patient_id}/{cycle_id}: {e}\")\n",
    "    \n",
    "    return pd.DataFrame(structure_info)\n",
    "\n",
    "# Run structure analysis\n",
    "structure_df = scan_data_structure()\n",
    "if structure_df is not None:\n",
    "    print(f\"\\nStructure analysis complete: {len(structure_df)} cycles found\")\n",
    "    print(structure_df.head())\n",
    "else:\n",
    "    print(\"Structure analysis failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATA STRUCTURE SUMMARY ===\n",
      "Total cycles: 587\n",
      "Unique patients: 20\n",
      "Cycles per patient: 29.4\n",
      "\n",
      "=== PHASE COUNT DISTRIBUTION ===\n",
      "10 phases: 587 cycles (100.0%)\n",
      "\n",
      "=== VOLUME SHAPES ===\n",
      "(50, 512, 512): 505 cycles (86.0%)\n",
      "(118, 512, 512): 3 cycles (0.5%)\n",
      "(113, 512, 512): 3 cycles (0.5%)\n",
      "(115, 512, 512): 3 cycles (0.5%)\n",
      "(125, 512, 512): 3 cycles (0.5%)\n",
      "(117, 512, 512): 3 cycles (0.5%)\n",
      "(104, 512, 512): 3 cycles (0.5%)\n",
      "(110, 512, 512): 3 cycles (0.5%)\n",
      "(114, 512, 512): 3 cycles (0.5%)\n",
      "(103, 512, 512): 3 cycles (0.5%)\n",
      "(107, 512, 512): 2 cycles (0.3%)\n",
      "(133, 512, 512): 2 cycles (0.3%)\n",
      "(94, 512, 512): 2 cycles (0.3%)\n",
      "(137, 512, 512): 2 cycles (0.3%)\n",
      "(132, 512, 512): 2 cycles (0.3%)\n",
      "(99, 512, 512): 2 cycles (0.3%)\n",
      "(88, 512, 512): 2 cycles (0.3%)\n",
      "(122, 512, 512): 2 cycles (0.3%)\n",
      "(142, 512, 512): 2 cycles (0.3%)\n",
      "(91, 512, 512): 2 cycles (0.3%)\n",
      "(84, 512, 512): 2 cycles (0.3%)\n",
      "(105, 512, 512): 2 cycles (0.3%)\n",
      "(131, 512, 512): 1 cycles (0.2%)\n",
      "(168, 512, 512): 1 cycles (0.2%)\n",
      "(158, 512, 512): 1 cycles (0.2%)\n",
      "(101, 512, 512): 1 cycles (0.2%)\n",
      "(100, 512, 512): 1 cycles (0.2%)\n",
      "(151, 512, 512): 1 cycles (0.2%)\n",
      "(112, 512, 512): 1 cycles (0.2%)\n",
      "(119, 512, 512): 1 cycles (0.2%)\n",
      "(111, 512, 512): 1 cycles (0.2%)\n",
      "(109, 512, 512): 1 cycles (0.2%)\n",
      "(136, 512, 512): 1 cycles (0.2%)\n",
      "(93, 512, 512): 1 cycles (0.2%)\n",
      "(92, 512, 512): 1 cycles (0.2%)\n",
      "(128, 512, 512): 1 cycles (0.2%)\n",
      "(135, 512, 512): 1 cycles (0.2%)\n",
      "(85, 512, 512): 1 cycles (0.2%)\n",
      "(76, 512, 512): 1 cycles (0.2%)\n",
      "(89, 512, 512): 1 cycles (0.2%)\n",
      "(123, 512, 512): 1 cycles (0.2%)\n",
      "(120, 512, 512): 1 cycles (0.2%)\n",
      "(90, 512, 512): 1 cycles (0.2%)\n",
      "(77, 512, 512): 1 cycles (0.2%)\n",
      "(149, 512, 512): 1 cycles (0.2%)\n",
      "(147, 512, 512): 1 cycles (0.2%)\n",
      "(82, 512, 512): 1 cycles (0.2%)\n",
      "(121, 512, 512): 1 cycles (0.2%)\n",
      "(126, 512, 512): 1 cycles (0.2%)\n",
      "(106, 512, 512): 1 cycles (0.2%)\n",
      "(98, 512, 512): 1 cycles (0.2%)\n",
      "(2, 512, 512): 1 cycles (0.2%)\n",
      "(102, 512, 512): 1 cycles (0.2%)\n",
      "\n",
      "=== FILE SIZE STATISTICS ===\n",
      "Total data size: 172060.7 MB\n",
      "Average cycle size: 293.1 MB\n",
      "Size range: 10.0 - 840.0 MB\n"
     ]
    }
   ],
   "source": [
    "# Basic statistics\n",
    "if structure_df is not None:\n",
    "    print(\"=== DATA STRUCTURE SUMMARY ===\")\n",
    "    print(f\"Total cycles: {len(structure_df)}\")\n",
    "    print(f\"Unique patients: {structure_df['patient_id'].nunique()}\")\n",
    "    print(f\"Cycles per patient: {len(structure_df) / structure_df['patient_id'].nunique():.1f}\")\n",
    "    \n",
    "    print(\"\\n=== PHASE COUNT DISTRIBUTION ===\")\n",
    "    phase_counts = structure_df['num_phases'].value_counts().sort_index()\n",
    "    for phases, count in phase_counts.items():\n",
    "        print(f\"{phases} phases: {count} cycles ({count/len(structure_df)*100:.1f}%)\")\n",
    "    \n",
    "    print(\"\\n=== VOLUME SHAPES ===\")\n",
    "    shape_counts = structure_df['volume_shape'].value_counts()\n",
    "    for shape, count in shape_counts.items():\n",
    "        print(f\"{shape}: {count} cycles ({count/len(structure_df)*100:.1f}%)\")\n",
    "    \n",
    "    print(\"\\n=== FILE SIZE STATISTICS ===\")\n",
    "    print(f\"Total data size: {structure_df['file_size_mb'].sum():.1f} MB\")\n",
    "    print(f\"Average cycle size: {structure_df['file_size_mb'].mean():.1f} MB\")\n",
    "    print(f\"Size range: {structure_df['file_size_mb'].min():.1f} - {structure_df['file_size_mb'].max():.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Motion Analysis Between Phases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing motion analysis on sample cycles...\n",
      "✓ 116_HM10395/116_HM10395_S106: avg_mse=0.0629\n",
      "✓ 116_HM10395/116_HM10395_S107: avg_mse=0.0541\n",
      "✓ 116_HM10395/116_HM10395_S301: avg_mse=0.0070\n",
      "\n",
      "Sample motion analysis completed: 3 cycles\n"
     ]
    }
   ],
   "source": [
    "def analyze_single_cycle_motion(patient_id, cycle_id):\n",
    "    \"\"\"Analyze motion patterns within a single breathing cycle\"\"\"\n",
    "    \n",
    "    cycle_path = CYCLES_ROOT / patient_id / cycle_id\n",
    "    \n",
    "    if not cycle_path.exists():\n",
    "        return None\n",
    "    \n",
    "    # Load all phases\n",
    "    phase_files = sorted(cycle_path.glob(\"phase_*.npy\"))\n",
    "    \n",
    "    if len(phase_files) != 10:\n",
    "        print(f\"Warning: {patient_id}/{cycle_id} has {len(phase_files)} phases, expected 10\")\n",
    "        return None\n",
    "    \n",
    "    phases = []\n",
    "    for phase_file in phase_files:\n",
    "        try:\n",
    "            volume = np.load(phase_file)\n",
    "            phases.append(volume)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {phase_file}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    # Calculate motion metrics\n",
    "    motion_analysis = {\n",
    "        'patient_id': patient_id,\n",
    "        'cycle_id': cycle_id,\n",
    "        'num_phases': len(phases),\n",
    "        'volume_shape': phases[0].shape\n",
    "    }\n",
    "    \n",
    "    # Motion between consecutive phases\n",
    "    consecutive_motion = []\n",
    "    \n",
    "    for i in range(len(phases)):\n",
    "        # Next phase (wrap around for last phase)\n",
    "        next_i = (i + 1) % len(phases)\n",
    "        \n",
    "        # Use middle slice for motion analysis\n",
    "        mid_slice = phases[i].shape[0] // 2\n",
    "        \n",
    "        slice1 = phases[i][mid_slice].astype(np.float32)\n",
    "        slice2 = phases[next_i][mid_slice].astype(np.float32)\n",
    "        \n",
    "        # Normalize\n",
    "        slice1_norm = (slice1 - slice1.mean()) / (slice1.std() + 1e-8)\n",
    "        slice2_norm = (slice2 - slice2.mean()) / (slice2.std() + 1e-8)\n",
    "        \n",
    "        # Calculate motion metrics\n",
    "        mse = mean_squared_error(slice1_norm.flatten(), slice2_norm.flatten())\n",
    "        mae = np.mean(np.abs(slice1_norm - slice2_norm))\n",
    "        \n",
    "        # Correlation\n",
    "        correlation = np.corrcoef(slice1_norm.flatten(), slice2_norm.flatten())[0, 1]\n",
    "        \n",
    "        consecutive_motion.append({\n",
    "            'phase_from': i,\n",
    "            'phase_to': next_i,\n",
    "            'mse': mse,\n",
    "            'mae': mae,\n",
    "            'correlation': correlation\n",
    "        })\n",
    "    \n",
    "    # Overall motion statistics\n",
    "    motion_analysis['consecutive_motion'] = consecutive_motion\n",
    "    motion_analysis['avg_mse'] = np.mean([m['mse'] for m in consecutive_motion])\n",
    "    motion_analysis['max_mse'] = np.max([m['mse'] for m in consecutive_motion])\n",
    "    motion_analysis['avg_mae'] = np.mean([m['mae'] for m in consecutive_motion])\n",
    "    motion_analysis['avg_correlation'] = np.mean([m['correlation'] for m in consecutive_motion])\n",
    "    \n",
    "    # Find phase with maximum motion\n",
    "    max_motion_idx = np.argmax([m['mse'] for m in consecutive_motion])\n",
    "    motion_analysis['max_motion_phase'] = max_motion_idx\n",
    "    \n",
    "    return motion_analysis\n",
    "\n",
    "# Test on a few cycles first\n",
    "print(\"Testing motion analysis on sample cycles...\")\n",
    "\n",
    "# Get first few cycles for testing\n",
    "sample_cycles = structure_df.head(3) if structure_df is not None else []\n",
    "\n",
    "sample_results = []\n",
    "for _, row in sample_cycles.iterrows():\n",
    "    result = analyze_single_cycle_motion(row['patient_id'], row['cycle_id'])\n",
    "    if result:\n",
    "        sample_results.append(result)\n",
    "        print(f\"✓ {row['patient_id']}/{row['cycle_id']}: avg_mse={result['avg_mse']:.4f}\")\n",
    "\n",
    "print(f\"\\nSample motion analysis completed: {len(sample_results)} cycles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Full Motion Analysis (Run Only if Sample Works)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample analysis successful. Running optimized motion analysis...\n",
      "Using 4 processes with batch size 36\n",
      "Total batches: 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|          | 0/17 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Even more optimized version with batch processing and memory efficiency\n",
    "import numpy as np\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import gc\n",
    "import time\n",
    "\n",
    "def analyze_batch_motion(batch_args):\n",
    "    \"\"\"Process a batch of cycles to reduce overhead\"\"\"\n",
    "    batch_results = []\n",
    "    \n",
    "    for patient_id, cycle_id in batch_args:\n",
    "        try:\n",
    "            result = analyze_single_cycle_motion(patient_id, cycle_id)\n",
    "            if result:\n",
    "                batch_results.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {patient_id}/{cycle_id}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return batch_results\n",
    "\n",
    "def create_batches(items, batch_size):\n",
    "    \"\"\"Split items into batches\"\"\"\n",
    "    for i in range(0, len(items), batch_size):\n",
    "        yield items[i:i + batch_size]\n",
    "\n",
    "# Ultra-optimized version\n",
    "if len(sample_results) > 0:\n",
    "    print(\"Sample analysis successful. Running optimized motion analysis...\")\n",
    "    \n",
    "    # Prepare arguments\n",
    "    cycle_args = [(row['patient_id'], row['cycle_id']) for _, row in structure_df.iterrows()]\n",
    "    \n",
    "    # Batch processing parameters\n",
    "    n_processes = max(1, int(cpu_count() * 0.75))\n",
    "    batch_size = max(1, len(cycle_args) // (n_processes * 4))  # 4 batches per process\n",
    "    \n",
    "    print(f\"Using {n_processes} processes with batch size {batch_size}\")\n",
    "    print(f\"Total batches: {len(list(create_batches(cycle_args, batch_size)))}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Create batches\n",
    "    batches = list(create_batches(cycle_args, batch_size))\n",
    "    \n",
    "    # Process batches in parallel\n",
    "    all_motion_results = []\n",
    "    \n",
    "    with Pool(processes=n_processes) as pool:\n",
    "        # Process batches\n",
    "        batch_results = list(tqdm(\n",
    "            pool.imap(analyze_batch_motion, batches),\n",
    "            total=len(batches),\n",
    "            desc=\"Processing batches\"\n",
    "        ))\n",
    "        \n",
    "        # Flatten results\n",
    "        for batch_result in batch_results:\n",
    "            all_motion_results.extend(batch_result)\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\nOptimized motion analysis completed: {len(all_motion_results)} cycles\")\n",
    "    print(f\"Processing time: {elapsed_time:.1f} seconds ({elapsed_time/60:.1f} minutes)\")\n",
    "    print(f\"Average time per cycle: {elapsed_time/len(all_motion_results):.2f} seconds\")\n",
    "    print(f\"Speedup: ~{n_processes:.1f}x faster than sequential\")\n",
    "    \n",
    "    # Memory cleanup\n",
    "    gc.collect()\n",
    "    \n",
    "    # Save results (same as before)\n",
    "    motion_summary = []\n",
    "    for result in all_motion_results:\n",
    "        motion_summary.append({\n",
    "            'patient_id': result['patient_id'],\n",
    "            'cycle_id': result['cycle_id'],\n",
    "            'num_phases': result['num_phases'],\n",
    "            'volume_shape': str(result['volume_shape']),\n",
    "            'avg_mse': result['avg_mse'],\n",
    "            'max_mse': result['max_mse'],\n",
    "            'avg_mae': result['avg_mae'],\n",
    "            'avg_correlation': result['avg_correlation'],\n",
    "            'max_motion_phase': result['max_motion_phase']\n",
    "        })\n",
    "    \n",
    "    motion_df = pd.DataFrame(motion_summary)\n",
    "    motion_df.to_csv(OUTPUT_DIR / \"motion_analysis.csv\", index=False)\n",
    "    \n",
    "else:\n",
    "    print(\"Sample analysis failed. Skipping full analysis.\")\n",
    "    motion_df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Motion Statistics Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'motion_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mmotion_df\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m      2\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=== MOTION ANALYSIS SUMMARY ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCycles analyzed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(motion_df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'motion_df' is not defined"
     ]
    }
   ],
   "source": [
    "if motion_df is not None:\n",
    "    print(\"=== MOTION ANALYSIS SUMMARY ===\")\n",
    "    print(f\"Cycles analyzed: {len(motion_df)}\")\n",
    "    \n",
    "    print(\"\\n=== MOTION STATISTICS ===\")\n",
    "    print(f\"Average MSE: {motion_df['avg_mse'].mean():.6f} ± {motion_df['avg_mse'].std():.6f}\")\n",
    "    print(f\"Average MAE: {motion_df['avg_mae'].mean():.6f} ± {motion_df['avg_mae'].std():.6f}\")\n",
    "    print(f\"Average Correlation: {motion_df['avg_correlation'].mean():.4f} ± {motion_df['avg_correlation'].std():.4f}\")\n",
    "    \n",
    "    print(\"\\n=== MOTION MAGNITUDE DISTRIBUTION ===\")\n",
    "    print(f\"Low motion (MSE < 0.01): {(motion_df['avg_mse'] < 0.01).sum()} cycles\")\n",
    "    print(f\"Medium motion (0.01 ≤ MSE < 0.1): {((motion_df['avg_mse'] >= 0.01) & (motion_df['avg_mse'] < 0.1)).sum()} cycles\")\n",
    "    print(f\"High motion (MSE ≥ 0.1): {(motion_df['avg_mse'] >= 0.1).sum()} cycles\")\n",
    "    \n",
    "    print(\"\\n=== PEAK MOTION PHASES ===\")\n",
    "    phase_counts = motion_df['max_motion_phase'].value_counts().sort_index()\n",
    "    for phase, count in phase_counts.items():\n",
    "        print(f\"Phase {phase}: {count} cycles ({count/len(motion_df)*100:.1f}%)\")\n",
    "    \n",
    "    print(\"\\n=== CORRELATION ANALYSIS ===\")\n",
    "    print(f\"High correlation (>0.9): {(motion_df['avg_correlation'] > 0.9).sum()} cycles\")\n",
    "    print(f\"Medium correlation (0.7-0.9): {((motion_df['avg_correlation'] >= 0.7) & (motion_df['avg_correlation'] <= 0.9)).sum()} cycles\")\n",
    "    print(f\"Low correlation (<0.7): {(motion_df['avg_correlation'] < 0.7).sum()} cycles\")\n",
    "else:\n",
    "    print(\"No motion analysis data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_data_quality():\n",
    "    \"\"\"Assess data quality for interpolation\"\"\"\n",
    "    \n",
    "    quality_report = {\n",
    "        'total_cycles': len(structure_df) if structure_df is not None else 0,\n",
    "        'valid_cycles': 0,\n",
    "        'issues': []\n",
    "    }\n",
    "    \n",
    "    if structure_df is None:\n",
    "        quality_report['issues'].append(\"No structure data available\")\n",
    "        return quality_report\n",
    "    \n",
    "    # Check for standard 10-phase cycles\n",
    "    standard_cycles = structure_df[structure_df['num_phases'] == 10]\n",
    "    quality_report['standard_10_phase_cycles'] = len(standard_cycles)\n",
    "    \n",
    "    if len(standard_cycles) != len(structure_df):\n",
    "        non_standard = structure_df[structure_df['num_phases'] != 10]\n",
    "        quality_report['issues'].append(f\"{len(non_standard)} cycles don't have 10 phases\")\n",
    "    \n",
    "    # Check for consistent shapes\n",
    "    shape_counts = structure_df['volume_shape'].value_counts()\n",
    "    most_common_shape = shape_counts.index[0]\n",
    "    standard_shape_cycles = structure_df[structure_df['volume_shape'] == most_common_shape]\n",
    "    quality_report['standard_shape_cycles'] = len(standard_shape_cycles)\n",
    "    quality_report['most_common_shape'] = most_common_shape\n",
    "    \n",
    "    if len(shape_counts) > 1:\n",
    "        quality_report['issues'].append(f\"{len(shape_counts)} different volume shapes found\")\n",
    "    \n",
    "    # Motion quality assessment\n",
    "    if motion_df is not None:\n",
    "        # Good motion: not too high (noisy) or too low (static)\n",
    "        good_motion_cycles = motion_df[\n",
    "            (motion_df['avg_mse'] > 0.001) & \n",
    "            (motion_df['avg_mse'] < 0.5) & \n",
    "            (motion_df['avg_correlation'] > 0.5)\n",
    "        ]\n",
    "        quality_report['good_motion_cycles'] = len(good_motion_cycles)\n",
    "        \n",
    "        # Very low motion (might be static)\n",
    "        static_cycles = motion_df[motion_df['avg_mse'] < 0.001]\n",
    "        if len(static_cycles) > 0:\n",
    "            quality_report['issues'].append(f\"{len(static_cycles)} cycles have very low motion (possibly static)\")\n",
    "        \n",
    "        # Very high motion (might be noisy)\n",
    "        noisy_cycles = motion_df[motion_df['avg_mse'] > 0.5]\n",
    "        if len(noisy_cycles) > 0:\n",
    "            quality_report['issues'].append(f\"{len(noisy_cycles)} cycles have very high motion (possibly noisy)\")\n",
    "    \n",
    "    # Final recommendation\n",
    "    if motion_df is not None:\n",
    "        usable_cycles = motion_df[\n",
    "            (motion_df['num_phases'] == 10) &\n",
    "            (motion_df['avg_mse'] > 0.001) &\n",
    "            (motion_df['avg_mse'] < 0.5) &\n",
    "            (motion_df['avg_correlation'] > 0.5)\n",
    "        ]\n",
    "        quality_report['recommended_for_interpolation'] = len(usable_cycles)\n",
    "    \n",
    "    return quality_report\n",
    "\n",
    "# Run quality assessment\n",
    "quality_report = assess_data_quality()\n",
    "\n",
    "print(\"=== DATA QUALITY ASSESSMENT ===\")\n",
    "print(f\"Total cycles found: {quality_report['total_cycles']}\")\n",
    "print(f\"Standard 10-phase cycles: {quality_report.get('standard_10_phase_cycles', 'N/A')}\")\n",
    "print(f\"Standard shape cycles: {quality_report.get('standard_shape_cycles', 'N/A')}\")\n",
    "print(f\"Most common shape: {quality_report.get('most_common_shape', 'N/A')}\")\n",
    "print(f\"Good motion cycles: {quality_report.get('good_motion_cycles', 'N/A')}\")\n",
    "print(f\"Recommended for interpolation: {quality_report.get('recommended_for_interpolation', 'N/A')}\")\n",
    "\n",
    "if quality_report['issues']:\n",
    "    print(\"\\n=== ISSUES FOUND ===\")\n",
    "    for issue in quality_report['issues']:\n",
    "        print(f\"⚠️  {issue}\")\n",
    "else:\n",
    "    print(\"\\n✓ No major issues found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Analysis Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all analysis results\n",
    "print(\"=== SAVING ANALYSIS RESULTS ===\")\n",
    "\n",
    "# Save structure analysis\n",
    "if structure_df is not None:\n",
    "    structure_df.to_csv(OUTPUT_DIR / \"structure_analysis.csv\", index=False)\n",
    "    print(f\"✓ Structure analysis saved to {OUTPUT_DIR / 'structure_analysis.csv'}\")\n",
    "\n",
    "# Save motion analysis (already saved above)\n",
    "if motion_df is not None:\n",
    "    print(f\"✓ Motion analysis saved to {OUTPUT_DIR / 'motion_analysis.csv'}\")\n",
    "\n",
    "# Save quality report\n",
    "with open(OUTPUT_DIR / \"quality_report.json\", 'w') as f:\n",
    "    json.dump(quality_report, f, indent=2, default=str)\n",
    "print(f\"✓ Quality report saved to {OUTPUT_DIR / 'quality_report.json'}\")\n",
    "\n",
    "# Create summary for implementation\n",
    "implementation_summary = {\n",
    "    'data_ready': quality_report.get('recommended_for_interpolation', 0) > 0,\n",
    "    'total_cycles': quality_report['total_cycles'],\n",
    "    'usable_cycles': quality_report.get('recommended_for_interpolation', 0),\n",
    "    'standard_shape': quality_report.get('most_common_shape', 'Unknown'),\n",
    "    'major_issues': quality_report['issues'],\n",
    "    'next_steps': []\n",
    "}\n",
    "\n",
    "if implementation_summary['data_ready']:\n",
    "    implementation_summary['next_steps'].append(\"Data quality is good for interpolation\")\n",
    "    implementation_summary['next_steps'].append(\"Can proceed with dataset creation\")\n",
    "    implementation_summary['next_steps'].append(\"Recommend starting with flow matching approach\")\n",
    "else:\n",
    "    implementation_summary['next_steps'].append(\"Data quality issues need to be addressed\")\n",
    "    implementation_summary['next_steps'].append(\"Review and clean data before interpolation\")\n",
    "\n",
    "with open(OUTPUT_DIR / \"implementation_summary.json\", 'w') as f:\n",
    "    json.dump(implementation_summary, f, indent=2, default=str)\n",
    "print(f\"✓ Implementation summary saved to {OUTPUT_DIR / 'implementation_summary.json'}\")\n",
    "\n",
    "print(\"\\n=== ANALYSIS COMPLETE ===\")\n",
    "print(f\"All results saved to: {OUTPUT_DIR}\")\n",
    "print(f\"Ready for interpolation: {implementation_summary['data_ready']}\")\n",
    "print(f\"Usable cycles: {implementation_summary['usable_cycles']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00366231",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CYCLE ANALYZER SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if structure_df is not None:\n",
    "    print(\"📊 DATASET OVERVIEW:\")\n",
    "    print(f\"   • Total cycles: {len(structure_df)}\")\n",
    "    print(f\"   • Unique patients: {structure_df['patient_id'].nunique()}\")\n",
    "    print(f\"   • Cycles per patient: {len(structure_df) / structure_df['patient_id'].nunique():.1f}\")\n",
    "    \n",
    "    print(\"\\n📐 DATA STRUCTURE:\")\n",
    "    print(f\"   • Standard 10-phase cycles: {quality_report.get('standard_10_phase_cycles', 'N/A')}\")\n",
    "    print(f\"   • Most common shape: {quality_report.get('most_common_shape', 'N/A')}\")\n",
    "    print(f\"   • Total data size: {structure_df['file_size_mb'].sum():.1f} MB\")\n",
    "\n",
    "if motion_df is not None:\n",
    "    print(\"\\n🔄 MOTION ANALYSIS:\")\n",
    "    print(f\"   • Average MSE: {motion_df['avg_mse'].mean():.6f}\")\n",
    "    print(f\"   • Average correlation: {motion_df['avg_correlation'].mean():.4f}\")\n",
    "    print(f\"   • Good motion cycles: {quality_report.get('good_motion_cycles', 'N/A')}\")\n",
    "\n",
    "print(\"\\n✅ QUALITY ASSESSMENT:\")\n",
    "print(f\"   • Recommended for interpolation: {quality_report.get('recommended_for_interpolation', 'N/A')}\")\n",
    "print(f\"   • Data ready: {implementation_summary['data_ready']}\")\n",
    "\n",
    "if quality_report.get('issues'):\n",
    "    print(\"\\n⚠️  ISSUES TO ADDRESS:\")\n",
    "    for issue in quality_report['issues']:\n",
    "        print(f\"   • {issue}\")\n",
    "\n",
    "print(\"\\n🚀 NEXT STEPS:\")\n",
    "for step in implementation_summary['next_steps']:\n",
    "    print(f\"   • {step}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flowmotion-N6nEB-HP-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
